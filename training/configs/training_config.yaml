model:
  model_id: "meta-llama/Llama-2-7b-chat-hf"

data:
  sample_size: 4000
  test_ratio: 0.05

quantization:
  load_in_4bit: True
  bnb_4bit_use_double_quant: False
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_compute_dtype: "float16"

training:
  epochs: 2
  learning_rate: 2e-4
  batch_size: 4
  gradient_accumulation_steps: 4